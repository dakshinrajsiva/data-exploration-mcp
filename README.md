# üìä Data Exploration MCP Server

> **Enterprise-grade data analytics platform with 28 specialized tools, advanced visualization capabilities, and production-ready performance optimization.**

[![Python](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![MCP](https://img.shields.io/badge/MCP-compatible-green.svg)](https://modelcontextprotocol.io/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)
[![GitHub stars](https://img.shields.io/github/stars/dakshinrajsiva/data-exploration-mcp.svg)](https://github.com/dakshinrajsiva/data-exploration-mcp/stargazers)
[![GitHub issues](https://img.shields.io/github/issues/dakshinrajsiva/data-exploration-mcp.svg)](https://github.com/dakshinrajsiva/data-exploration-mcp/issues)
[![Claude Desktop](https://img.shields.io/badge/Claude%20Desktop-compatible-orange.svg)](https://claude.ai/desktop)
[![Cursor IDE](https://img.shields.io/badge/Cursor%20IDE-compatible-purple.svg)](https://cursor.sh/)

## üöÄ **Quick Start**

### **Installation**

1. **Clone the repository**:
   ```bash
   git clone https://github.com/dakshinrajsiva/data-exploration-mcp.git
   cd data-exploration-mcp
   ```

2. **Install dependencies**:
   ```bash
   pip install -e .
   ```

3. **Test the installation**:
   ```bash
   python test_mcp_connection.py
   ```

### **üîß Configuration**

#### **Claude Desktop Setup**

1. **Locate Claude Desktop config file**:
   - **macOS**: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - **Windows**: `%APPDATA%\Claude\claude_desktop_config.json`
   - **Linux**: `~/.config/Claude/claude_desktop_config.json`

2. **Add MCP server configuration**:
   ```json
   {
     "mcpServers": {
       "data-exploration-mcp": {
         "command": "python",
         "args": ["/absolute/path/to/data-exploration-mcp/src/main.py"],
         "cwd": "/absolute/path/to/data-exploration-mcp",
         "env": {
           "PYTHONPATH": "/absolute/path/to/data-exploration-mcp"
         }
       }
     }
   }
   ```

3. **Replace paths with your actual installation directory**:
   ```bash
   # Get your current directory
   pwd
   # Use this path in the config above
   ```

4. **Restart Claude Desktop** and look for the üîå MCP icon

#### **Cursor IDE Setup**

1. **Open Cursor IDE** and go to Settings

2. **Navigate to Extensions ‚Üí MCP Servers**

3. **Add new MCP server** with these settings:
   - **Name**: `Data Exploration MCP`
   - **Command**: `python`
   - **Args**: `["/absolute/path/to/data-exploration-mcp/src/main.py"]`
   - **Working Directory**: `/absolute/path/to/data-exploration-mcp`

4. **Alternative: Use cursor_mcp_config.json**:
   ```json
   {
     "name": "data-exploration-mcp",
     "command": "python",
     "args": ["/absolute/path/to/data-exploration-mcp/src/main.py"],
     "cwd": "/absolute/path/to/data-exploration-mcp",
     "env": {
       "PYTHONPATH": "/absolute/path/to/data-exploration-mcp"
     }
   }
   ```

#### **üö® Troubleshooting Configuration**

**Common Issues:**

1. **"Server not found" error**:
   ```bash
   # Check Python path
   which python
   
   # Verify MCP server runs
   cd /path/to/data-exploration-mcp
   python src/main.py
   ```

2. **Import errors**:
   ```bash
   # Install missing dependencies
   pip install pandas numpy scipy
   
   # Or reinstall completely
   pip install -e .
   ```

3. **Path issues**:
   ```bash
   # Use absolute paths only
   realpath /path/to/data-exploration-mcp
   ```

**Verification Steps:**

1. **Test MCP connection**:
   ```bash
   python test_mcp_connection.py
   ```

2. **Check server logs** in Claude Desktop console

3. **Verify tools are available**: Look for 28 tools in the MCP interface

### **‚ö° Quick Test**

Once configured, try these commands in Claude or Cursor:

```
Use the discover_data tool on a CSV file to test the connection
```

```
Show me all available MCP tools
```

## üéØ **Key Features**

### **‚ö° Performance Optimization**
- **67% memory reduction** through intelligent optimization
- **3,000x+ speed improvements** via vectorized operations
- **Sub-50ms response times** for real-time analysis

### **üé® Advanced Visualization Suite**
- **39+ visualization types** across 6 specialized tools
- **Executive dashboards** with KPIs and business intelligence
- **Interactive plots** with statistical overlays and annotations

### **üìä Comprehensive Analytics**
- **28 specialized tools** for complete data exploration
- **Statistical analysis** with normality testing and distribution analysis
- **Machine learning readiness** assessment and feature engineering

### **üîß Production Ready**
- **Enterprise scalability** for large datasets
- **Robust error handling** with graceful failure recovery
- **Multiple export formats** (CSV, Parquet, JSON)

## üìã **Complete Tool Suite (28 Tools)**

### **üîç Data Discovery & Profiling**
| Tool | Purpose | Speed |
|------|---------|-------|
| `discover_data` | Instant dataset profiling | 30 seconds |
| `dataset_overview` | Comprehensive health check | Sub-second |
| `numeric_exploration` | Advanced numeric analysis | Sub-second |
| `distribution_checks` | Distribution visualization | Sub-second |

### **üìä Statistical Analysis**
| Tool | Purpose | Features |
|------|---------|----------|
| `skewness_analysis` | Distribution symmetry | Jarque-Bera test |
| `kurtosis_analysis` | Tail behavior analysis | Outlier propensity |
| `distribution_shape_analysis` | Comprehensive shape analysis | 3 normality tests |
| `correlation_analysis` | Variable relationships | Significance testing |

### **üé® Visualization Suite**
| Tool | Visualizations | Performance |
|------|----------------|-------------|
| `create_distribution_plots` | 20+ plots | 0.040s |
| `create_correlation_heatmap` | Advanced heatmaps | 0.051s |
| `create_time_series_plots` | Temporal analysis | 0.040s |
| `create_outlier_visualizations` | 10+ outlier plots | 0.037s |
| `create_business_intelligence_dashboard` | Executive dashboards | 0.043s |
| `create_advanced_scatter_matrix` | Multi-variable exploration | Sub-second |

### **‚ö° Performance Optimization**
| Tool | Achievement | Use Case |
|------|-------------|----------|
| `optimize_memory` | 67% reduction | Large datasets |
| `export_optimized_dataset` | Format optimization | Pipeline integration |
| `export_vectorized_dataset` | High performance | Real-time processing |
| `performance_benchmarking` | Detailed metrics | Optimization validation |

### **ü§ñ Machine Learning**
| Tool | Purpose | Output |
|------|---------|--------|
| `ml_readiness_assessment` | ML scoring | Model recommendations |
| `advanced_feature_engineering` | Feature creation | Enhanced datasets |

### **üìà Advanced Analytics**
| Tool | Purpose | Features |
|------|---------|----------|
| `temporal_analysis` | Time series analysis | Trends & seasonality |
| `scatter_plots` | Relationship analysis | Statistical annotations |
| `full_exploration_report` | Complete EDA | All analyses combined |
| `optimized_analysis_workflow` | Production pipeline | End-to-end processing |

### **üéØ Workflow Management**
| Tool | Purpose | Features |
|------|---------|----------|
| `start_guided_analysis` | Intelligent exploration | Context-aware suggestions |
| `continue_analysis` | Progressive analysis | Building insights |
| `executive_dashboard` | C-level reporting | Strategic insights |
| `explain_methodology` | Analysis transparency | Educational documentation |

## üèÉ‚Äç‚ôÇÔ∏è **Usage Examples**

### **Quick Data Exploration**
```python
# 1. Discover your data (30-second analysis)
await handle_call_tool('discover_data', {'file_path': 'your_data.csv'})

# 2. Get comprehensive overview
await handle_call_tool('dataset_overview', {'file_path': 'your_data.csv'})

# 3. Analyze numeric variables
await handle_call_tool('numeric_exploration', {'file_path': 'your_data.csv'})
```

### **Performance-Optimized Workflow**
```python
# 1. Optimize memory usage (67% reduction)
await handle_call_tool('optimize_memory', {'file_path': 'large_dataset.csv'})

# 2. Run optimized analysis pipeline
await handle_call_tool('optimized_analysis_workflow', {'file_path': 'large_dataset.csv'})

# 3. Export optimized dataset
await handle_call_tool('export_optimized_dataset', {
    'file_path': 'large_dataset.csv',
    'format': 'parquet'
})
```

### **Advanced Visualization**
```python
# 1. Create distribution plots
await handle_call_tool('create_distribution_plots', {
    'file_path': 'data.csv',
    'plot_types': ['histogram', 'boxplot', 'violin']
})

# 2. Generate correlation heatmap
await handle_call_tool('create_correlation_heatmap', {
    'file_path': 'data.csv',
    'method': 'both',
    'cluster_variables': True
})

# 3. Build executive dashboard
await handle_call_tool('create_business_intelligence_dashboard', {
    'file_path': 'data.csv',
    'business_context': 'operations'
})
```

### **Machine Learning Preparation**
```python
# 1. Assess ML readiness
await handle_call_tool('ml_readiness_assessment', {'file_path': 'data.csv'})

# 2. Engineer features
await handle_call_tool('advanced_feature_engineering', {'file_path': 'data.csv'})

# 3. Export for ML pipeline
await handle_call_tool('export_vectorized_dataset', {
    'file_path': 'data.csv',
    'format': 'parquet'
})
```

## üìä **Performance Benchmarks**

### **Speed Metrics**
- **Data Discovery**: 30-second comprehensive profiling
- **Memory Optimization**: 67% average reduction
- **Visualization**: 39+ plots in <1 second
- **Statistical Analysis**: Sub-50ms response times

### **Scalability**
- **Dataset Size**: Enterprise-scale handling
- **Memory Efficiency**: Intelligent optimization algorithms
- **Processing Speed**: Vectorized operations throughout
- **Export Performance**: Format-optimized outputs

## üé® **Visualization Capabilities**

### **Distribution Analysis**
- **Histograms** with statistical overlays
- **Box plots** with outlier identification
- **Violin plots** combining density and quartiles
- **20+ visualizations** per analysis

### **Correlation Analysis**
- **Advanced heatmaps** with clustering
- **Statistical significance** indicators
- **Variable relationship** mapping
- **Multicollinearity** detection

### **Business Intelligence**
- **Executive dashboards** with KPIs
- **Performance monitoring** cards
- **Strategic insights** generation
- **Trend analysis** components

### **Outlier Detection**
- **Multi-method detection** (Z-score, IQR, Isolation Forest)
- **Interactive visualizations** with highlighting
- **Statistical annotations** and explanations
- **Business impact** assessment

## üîß **Technical Architecture**

### **Core Technologies**
- **Python 3.8+** with pandas, numpy, scipy
- **MCP Protocol** for Claude/Cursor integration
- **Async/await** for high-performance processing
- **JSON serialization** with custom handlers

### **Performance Features**
- **Memory optimization** algorithms
- **Vectorized operations** throughout
- **Intelligent caching** for repeated operations
- **Format-specific** export optimization

### **Integration**
- **Claude Desktop**: Native MCP support
- **Cursor IDE**: Seamless integration
- **API Compatible**: Standard JSON responses
- **Export Flexible**: Multiple format support

## üìà **Use Cases**

### **üîç For Data Scientists**
- **Rapid exploration** of unknown datasets
- **Advanced statistical** analysis and testing
- **Visualization creation** for presentations
- **ML preparation** and feature engineering

### **üíº For Business Analysts**
- **Executive dashboards** for stakeholder reporting
- **Performance monitoring** with KPIs
- **Trend analysis** and forecasting support
- **Business intelligence** insights

### **‚ö° For Data Engineers**
- **Memory optimization** for cost reduction
- **Performance benchmarking** and monitoring
- **Pipeline integration** with optimized exports
- **Scalability assessment** and optimization

### **üè¢ For Organizations**
- **Cost reduction** through resource optimization
- **Time savings** via automated workflows
- **Quality assurance** through comprehensive validation
- **Competitive advantage** with advanced analytics

## üõ†Ô∏è **Development**

### **Project Structure**
```
data-exploration-mcp/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # MCP server entry point
‚îÇ   ‚îú‚îÄ‚îÄ simple_mcp_server.py # Core server implementation
‚îÇ   ‚îî‚îÄ‚îÄ utils/               # Utility modules
‚îú‚îÄ‚îÄ test_dataset.csv         # Sample dataset for testing
‚îú‚îÄ‚îÄ test_mcp_connection.py   # Server testing script
‚îú‚îÄ‚îÄ pyproject.toml          # Project configuration
‚îî‚îÄ‚îÄ README.md               # This file
```

### **Testing**
```bash
# Test MCP server functionality
python test_mcp_connection.py

# Test specific tools
python -c "
import asyncio
from src.simple_mcp_server import handle_call_tool
result = asyncio.run(handle_call_tool('discover_data', {'file_path': 'test_dataset.csv'}))
print(result)
"
```

### **Contributing**
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed guidelines.

## üìö **Documentation**

- **[Complete Functionalities Overview](MCP_FUNCTIONALITIES_OVERVIEW.md)** - Detailed documentation of all 28 tools
- **[Quick Reference](MCP_QUICK_REFERENCE.md)** - Fast lookup guide for all tools
- **[Installation Guide](INSTALLATION.md)** - Detailed setup instructions
- **[Setup Guide](MCP_SETUP_GUIDE.md)** - MCP configuration for Claude/Cursor
- **[Changelog](CHANGELOG.md)** - Version history and updates
- **[Contributing Guide](CONTRIBUTING.md)** - How to contribute to the project

## üåü **Community**

### **Join the Community**
- ‚≠ê **Star this repository** if you find it useful
- üêõ **Report issues** or request features via GitHub Issues
- üí¨ **Join discussions** about data analysis and MCP development
- ü§ù **Contribute** improvements and new features

### **Show Your Support**
```bash
# Give us a star on GitHub
https://github.com/dakshinrajsiva/data-exploration-mcp

# Share with your network
# Tweet about your data analysis success stories using this MCP server
```

### **Featured Use Cases**
- **Financial Analysis**: Risk assessment and portfolio optimization
- **Healthcare Data**: Patient outcome analysis and clinical research
- **Marketing Analytics**: Customer segmentation and campaign optimization
- **Operations Research**: Supply chain optimization and performance monitoring
- **Academic Research**: Statistical analysis and data visualization for publications

## üìã **System Requirements**

### **Minimum Requirements**
- **Python**: 3.8 or higher
- **Memory**: 4GB RAM (8GB+ recommended for large datasets)
- **Storage**: 500MB free space
- **OS**: Windows 10+, macOS 10.14+, or Linux (Ubuntu 18.04+)

### **Recommended Setup**
- **Python**: 3.10+ for optimal performance
- **Memory**: 16GB+ RAM for enterprise datasets
- **Storage**: 2GB+ for data exports and caching
- **CPU**: Multi-core processor for vectorized operations

### **Dependencies**
```bash
# Core dependencies (auto-installed)
pandas >= 1.3.0
numpy >= 1.21.0
scipy >= 1.7.0
mcp >= 0.1.0

# Optional for enhanced features
matplotlib >= 3.5.0  # For visualization exports
seaborn >= 0.11.0    # For advanced plotting
```

## üîß **API Reference**

### **Core Tool Categories**

#### **üìä Data Discovery**
```python
# Quick dataset profiling
discover_data(file_path="data.csv")

# Comprehensive overview
dataset_overview(file_path="data.csv")

# Statistical analysis
numeric_exploration(file_path="data.csv")
```

#### **üé® Visualization**
```python
# Distribution plots
create_distribution_plots(
    file_path="data.csv",
    plot_types=["histogram", "boxplot", "violin"]
)

# Correlation heatmap
create_correlation_heatmap(
    file_path="data.csv",
    method="both",
    cluster_variables=True
)

# Executive dashboard
create_business_intelligence_dashboard(
    file_path="data.csv",
    business_context="operations"
)
```

#### **‚ö° Performance**
```python
# Memory optimization
optimize_memory(file_path="data.csv")

# Complete optimized workflow
optimized_analysis_workflow(file_path="data.csv")

# Performance benchmarking
performance_benchmarking(file_path="data.csv")
```

### **Response Format**
All tools return standardized JSON responses:
```json
{
  "status": "success",
  "data": { /* tool-specific results */ },
  "insights": ["Human-readable insights"],
  "business_value": ["Business impact statements"],
  "processing_time_seconds": 0.045,
  "next_suggestions": ["Recommended follow-up actions"]
}
```

## üí° **Advanced Examples**

### **Complete Data Analysis Workflow**
```python
# 1. Start with data discovery
result1 = await discover_data({"file_path": "sales_data.csv"})

# 2. Optimize for performance
result2 = await optimize_memory({"file_path": "sales_data.csv"})

# 3. Statistical analysis
result3 = await distribution_shape_analysis({
    "file_path": "sales_data.csv",
    "include_normality_tests": True
})

# 4. Visualizations
result4 = await create_business_intelligence_dashboard({
    "file_path": "sales_data.csv",
    "business_context": "sales",
    "include_kpis": True
})

# 5. Export optimized data
result5 = await export_optimized_dataset({
    "file_path": "sales_data.csv",
    "format": "parquet"
})
```

### **Interactive Analysis Session**
```python
# Start guided analysis
session = await start_guided_analysis({
    "file_path": "customer_data.csv",
    "analysis_goal": "customer_segmentation"
})

# Continue based on findings
next_step = await continue_analysis({
    "previous_context": session["context"],
    "user_interest": "correlation_patterns"
})
```

### **Production Pipeline Integration**
```python
# Memory-optimized pipeline
pipeline_result = await optimized_analysis_workflow({
    "file_path": "production_data.csv",
    "export_format": "parquet",
    "include_visualizations": True,
    "business_context": "operations"
})

# Extract key metrics
memory_savings = pipeline_result["optimization_results"]["memory_reduction_percentage"]
processing_time = pipeline_result["processing_time_seconds"]
insights = pipeline_result["insights"]
```

## ü§ù **Support**

### **Getting Help**
- **Issues**: Report bugs and request features via [GitHub Issues](https://github.com/dakshinrajsiva/data-exploration-mcp/issues)
- **Discussions**: Ask questions in [GitHub Discussions](https://github.com/dakshinrajsiva/data-exploration-mcp/discussions)
- **Documentation**: Check the comprehensive guides in this repository

### **Common Issues & Solutions**
- **Memory errors**: Use `optimize_memory` tool first for large datasets
- **Slow performance**: Enable vectorization with `optimized_analysis_workflow`
- **Missing dependencies**: Run `pip install -e .` to install all requirements
- **Configuration issues**: Check the troubleshooting section above
- **Import errors**: Verify Python path and virtual environment setup

## üìÑ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üéâ **Acknowledgments**

- Built with the **Model Context Protocol (MCP)** for seamless AI integration
- Optimized for **Claude Desktop** and **Cursor IDE**
- Designed for **enterprise-scale** data analysis workflows

---

**üöÄ Ready to transform your data analysis workflow with enterprise-grade performance and comprehensive insights? Get started with the Data Exploration MCP Server today!**

## üìä **Quick Stats**

| Metric | Value |
|--------|-------|
| **Total Tools** | 28 specialized tools |
| **Visualization Types** | 39+ plot types |
| **Memory Optimization** | Up to 67% reduction |
| **Speed Improvement** | 3,000x+ via vectorization |
| **Response Time** | Sub-50ms average |
| **Dataset Support** | Enterprise-scale |
| **Export Formats** | CSV, Parquet, JSON |
| **Integration** | Claude, Cursor, API |